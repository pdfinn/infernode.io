---
---

<section id="vision" class="px-6 py-24 lg:py-32">
  <div class="mx-auto max-w-3xl">
    <p class="mb-4 text-sm font-medium tracking-widest uppercase text-accent">The Problem</p>
    <h2 class="mb-12 text-3xl font-bold tracking-tight sm:text-4xl">
      AI agents are powerful.<br />That's the problem.
    </h2>

    <div class="space-y-6 text-lg leading-relaxed text-text-muted">
      <p>
        An AI agent that can read your files, call APIs, and spawn sub-processes is
        extraordinarily useful. It is also extraordinarily dangerous when it shares your
        operating system, your credentials, and your network access.
      </p>

      <p>
        Today's answer is guardrails: sandboxes bolted on top, permission dialogs, policy
        layers you <em>hope</em> hold. The agent runs inside your world and you trust the
        cage. History suggests this is unwise.
      </p>

      <p class="text-text text-xl font-medium">
        InferNode inverts the model entirely.
      </p>

      <p>
        Each agent gets its own namespace&mdash;a view of the world constructed from only the
        resources you explicitly mount. There is no cage because there is nothing to cage. The
        agent cannot access your credentials, your email, or your SSH keys because in its
        namespace, <strong class="text-text">those things do not exist</strong>.
      </p>

      <p>
        This changes everything. A provably contained agent is an agent you can trust with
        real autonomy. More autonomy means more productivity. And that is the point: security
        is not a constraint on the human-AI team. It is <strong class="text-text">the
        foundation that makes the team possible</strong>.
      </p>

      <p>
        <a href="/manifesto" class="text-accent transition hover:text-accent-glow">
          Read the full manifesto &rarr;
        </a>
      </p>
    </div>
  </div>
</section>
